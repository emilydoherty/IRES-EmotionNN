{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Dataset CEAP-360VR with pandas and scikit-learn\n",
    "\n",
    "This notebook loads and preprocesses the dataset `CEAP-360VR` [GitHub repo](https://github.com/luiseduve/CEAP-360VR-Dataset) described in the paper:\n",
    "\n",
    "*CEAP-360VR: A Continuous Physiological and Behavioral Emotion Annotation Dataset for 360 VR Videos* [(DOI)](10.1109/TMM.2021.3124080)\n",
    "\n",
    "*Description:* \n",
    "\n",
    "1. A class was created to load the individual Json files in a structured way through the index file `data_tree_index.json`. Similarly, demographics and video stimuli information are stored in two .csv files. The `Frame` data was used as main data source.\n",
    "2. The sampling frequency is different among data modalities, they were normalized to 30Hz for all videos (Video1 was at 25Hz). Moreover, we loaded `Raw` IBI to generate new signals `IBI_R_Peaks` indicating with a 1 when a heart-rate beat was detected. This information is useful for HRV analysis.\n",
    "3. Finally, the dataset is combined in a single dataframe containing all data @30Hz, without missing values and with target class labels to be used in classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ceap_loader\n",
    "\n",
    "# Libs for data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the files generated from this notebook are in a subfolder with this name\n",
    "STR_DATASET = \"ceap_example/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_path_temp(filename, subfolders=\"\", extension=\".csv\"):\n",
    "    # Function to generate temporary files easier\n",
    "    TEMP_FOLDER_NAME = \"./temp/\"\n",
    "    return ceap_loader.generate_complete_path(filename, \\\n",
    "                                        main_folder=TEMP_FOLDER_NAME, \\\n",
    "                                        subfolders=STR_DATASET+subfolders, \\\n",
    "                                        file_extension=extension)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1) Loading the dataset as CSV\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the root folder of the dataset with respect to the notebook\n",
    "dataset_root_folder = \"../../../CEAP-360VR/\"\n",
    "print(dataset_root_folder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class `DatasetCEAP()` generates three files in the same folder of the dataset:\n",
    "1. `data_tree_index.json`: The index containing the relative paths of the data files, grouped per data type (*Annotations, Behavior, Physio*), processing level (*Raw, Transformed, Frame*) and participant (*From 1 to 32*).\n",
    "2. `demographics_info_summary.csv`: A table that summarizes, per participant, the demographic information, subjective ratings from the questionnaires, and when the participant watched each of the 8 stimuli videos.\n",
    "3. `video_info_summary.csv`: A table describing the metadata from the video as in the folder `1_Stimuli`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_manager = ceap_loader.DatasetCEAP(dataset_root_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the stimuli data\n",
    "data_manager.stimuli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the demographics dataframe\n",
    "data_manager.demographics.head(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting data from one participant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters of data to load\n",
    "pid = 1         # 1-32\n",
    "typ = \"Physio\"  # [\"Annotations\", \"Behavior\", \"Physio\"]\n",
    "prep = \"Frame\"    # [\"Raw\", \"Transformed\", \"Frame\"]\n",
    "\n",
    "# Load data \n",
    "data_loaded = data_manager.load_data_from_participant(pid,typ,prep)\n",
    "data_loaded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots all data from a participant, dividing per videoId and data type.\n",
    "ceap_loader.plot_all_data_from_participant(data_loaded)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting and saving to PNG file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from a new participant\n",
    "data_loaded = data_manager.load_data_from_participant(5,\"Annotations\",\"Raw\")\n",
    "\n",
    "# Create the plot\n",
    "ceap_loader.plot_all_data_from_participant(data_loaded)\n",
    "\n",
    "# Save the plot in a custom folder (in this case \"./temp/\")\n",
    "save_path_plot = gen_path_temp(f\"plot_test\",subfolders=\"plots/\",extension=\".png\")\n",
    "fig = plt.gcf()\n",
    "fig.savefig(save_path_plot, dpi=300)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the data from all file in the dataset\n",
    "\n",
    "**Uncomment if needed** The cell below takes **>2 hours** plotting the whole dataset, per loaded file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Generate plots per data type and to visualize all the data per participant\n",
    "# for typ in data_manager.LIST_DATA_TYPES:\n",
    "#     for prep in data_manager.LIST_PROCESSING_LEVELS:\n",
    "#         for pid in range(1,33):\n",
    "#             data_loaded = data_manager.load_data_from_participant(pid,typ,prep)\n",
    "#             ceap_loader.plot_all_data_from_participant(data_loaded)\n",
    "#             save_path_plot = gen_path_temp(f\"{prep}/Participant{pid}_{typ}\",subfolders=\"plots/\",extension=\".png\")\n",
    "#             fig = plt.gcf()\n",
    "#             fig.savefig(save_path_plot, dpi=200)\n",
    "#             plt.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a CSV with data of interest\n",
    "\n",
    "In the example below, we load the data from all participants, all data types, and processing level `Frame`. \n",
    "\n",
    "Creating a CSV file for the whole dataset produces a CSV file of `~800MB`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Participants IDS\n",
    "PARTICIPANTS_IDS = np.arange(1,33)\n",
    "# Load data Annotations, Behavior, and Physio\n",
    "DATA_GROUPS = data_manager.LIST_DATA_TYPES\n",
    "# Load the Raw, Transformed, or Frame (resampled) data processing\n",
    "DATA_PROCESSING_LEVELS = data_manager.LIST_PROCESSING_LEVELS\n",
    "print(f\"DATA_GROUPS={DATA_GROUPS}, DATA_PROCESSING_LEVELS={DATA_PROCESSING_LEVELS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load or create dataframe with statistics of initial dataset\n",
    "df_ceap = None\n",
    "\n",
    "# Load all data resampled by frame\n",
    "for pid in PARTICIPANTS_IDS:     # Which participants to load\n",
    "    for dttype in DATA_GROUPS:   # Which data type to load\n",
    "        for prep in [\"Frame\"]:   # Which processing level\n",
    "            df_single_file = data_manager.load_data_from_participant(pid, dttype, prep)\n",
    "            df_ceap = df_single_file if (df_ceap is None) else pd.concat([df_ceap, df_single_file], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ceap[ df_ceap[\"data_type\"]==\"Physio\" ].isna().sum(axis=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `IBI` data is not sampled at the same frequency than the other physiological signals. Therefore, there will be missing values if you load `Physio` data.\n",
    "\n",
    "To load a cleaner version of the data, use the parameter `clean_physio = True` in the function `load_data_from_participant()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where the compiled dataset will be stored\n",
    "DATASET_POSTPROCESSED_FILENAME = gen_path_temp(\"Dataset_CEAP\", extension=\".csv\")\n",
    "# Load or create dataframe with statistics of initial dataset\n",
    "df_ceap = None\n",
    "try:\n",
    "    df_ceap = pd.read_csv(DATASET_POSTPROCESSED_FILENAME)\n",
    "    print(\"Data loaded from file\")\n",
    "except:\n",
    "    print(\"Creating file\")\n",
    "    # Load all data resampled by frame\n",
    "    for pid in PARTICIPANTS_IDS:     # Which participants to load\n",
    "        for dttype in DATA_GROUPS:   # Which data type to load\n",
    "            for prep in [\"Frame\"]:   # Which processing level\n",
    "                df_single_file = data_manager.load_data_from_participant(pid, dttype, prep, clean_physio=True)\n",
    "                df_ceap = df_single_file if (df_ceap is None) else pd.concat([df_ceap, df_single_file], axis=0)\n",
    "        \n",
    "    # Saving .csv\n",
    "    df_ceap.to_csv(DATASET_POSTPROCESSED_FILENAME, index=False)\n",
    "\n",
    "print(f\"\\n\\tFinished creating files {DATASET_POSTPROCESSED_FILENAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The physiological features do not have missing values\n",
    "df_ceap[ df_ceap[\"data_type\"]==\"Physio\" ].isna().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The columns with missing values can be dismissed to load the corresponding data type\n",
    "df_ceap[ df_ceap[\"data_type\"]==\"Physio\" ].dropna(axis=1, how=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another example showing only the annotations\n",
    "df_ceap[ df_ceap[\"data_type\"]==\"Annotations\" ].dropna(axis=1, how=\"all\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Preprocessed data\n",
    "---\n",
    "\n",
    "Returns the data from `Frame` with the following modifications:\n",
    "  - the data from the `VideoID=1` was upsampled from 25 to 30Hz, to make consistent the FPS with the other stimuli videos `VideoID=2..8`.\n",
    "  - Raw `IBI` was loaded to calculate HRV at 30Hz, and attached to the previous dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dictionary below can be used to recover the column names per data type\n",
    "COLS_PER_DATA_TYPE = {\n",
    "            'Annotations': ['Valence', 'Arousal'],\n",
    "            'Behavior': ['HM_Pitch', 'HM_Yaw', 'EM_Pitch', 'EM_Yaw', 'LEM_Pitch', 'LEM_Yaw', 'REM_Pitch', 'REM_Yaw', 'LPD_PD', 'RPD_PD'],\n",
    "            'Physio': ['ACC_ACC_X', 'ACC_ACC_Y', 'ACC_ACC_Z', 'SKT_SKT', 'EDA_EDA', 'BVP_BVP', 'HR_HR', 'IBI_R_Peaks']\n",
    "        }\n",
    "\n",
    "# Array with main column names in the dataset. Used to filter main columns in the dataset\n",
    "participant_colname = data_manager.K_PARTICIPANT\n",
    "ts_colname = data_manager.K_TIMESTAMP\n",
    "video_colname = data_manager.K_VIDEO\n",
    "basic_cols = [\"data_type\",\"processing_level\", participant_colname, video_colname, ts_colname]\n",
    "\n",
    "# Constant sampling frequency to be applied to data from Video1 and to transform IBI to peaks.\n",
    "RESAMPLING_FREQUENCY = 30       # What is the sampling frequency of the peaks array?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supersample `Frame` data from VideoID=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ceap = ceap_loader.resample_dataframes_from_video1(df_ceap, ts_colname=ts_colname)\n",
    "df_ceap.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the timestamps have the same 1800 timestamps, for 60 seconds of data @ 30Hz\n",
    "timestamps_reference = df_ceap.TimeStamp.unique()\n",
    "print(timestamps_reference)\n",
    "timestamps_reference.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ceap.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Raw IBI to R-peaks array and attach to dataset\n",
    "\n",
    "Next, we load the raw IBI data to extract a time series @ 30Hz that contains when a heart beat (or IBI peak) occurred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all raw IBI to generate HRV data\n",
    "data_IBI_raw = None\n",
    "for pid in PARTICIPANTS_IDS:\n",
    "    df_single_file = data_manager.load_data_from_participant(pid,\"Physio\",\"Raw\")\n",
    "    data_IBI_raw = df_single_file if (data_IBI_raw is None) else pd.concat([data_IBI_raw, df_single_file], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the column containing IBI data (this column will be removed and replaced by R-peaks)\n",
    "ibi_colname = \"IBI_IBI\"\n",
    "r_peaks_colname = \"IBI_R_Peaks\" # Name that will be used after transforming IBI into R-peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_peaks_from_IBI(df, FS = 30, ibi_colname=\"IBI_IBI\", output_colname=\"IBI_R_Peaks\"):\n",
    "    \"\"\"\n",
    "    Given a dataframe `df` containing irregular physiological \n",
    "    features from interbeat intervals with column name:`IBI_IBI`.\n",
    "    This function returns another dataframe containing 60 seconds of\n",
    "    data at the same sampling rate than the rest of the dataset\n",
    "    preprocessed as `Frame`.\n",
    "\n",
    "    This series contains the position of the `peaks` as `1`, and\n",
    "    the rest of the array contains zeros. The returned dataframe can be\n",
    "    directly used directly in neurokit2 package to extract HRV features:\n",
    "     - `neurokit2.hrv(peaks, sampling_rate=FS)`\n",
    "    \"\"\"\n",
    "    # The first IBI allows to regenerate a new peak right after the first IBI\n",
    "    first_beat_time = df.index[0]\n",
    "    first_beat_time = first_beat_time - df.iloc[0][0]\n",
    "    if(first_beat_time>=0):\n",
    "        df.loc[first_beat_time] = 0\n",
    "    df.sort_index(inplace=True)\n",
    "    \n",
    "    # Generate an zero-array that will contain the R-peaks as 1's at a specific sampling frequency `FPS`\n",
    "    MAXIMUM_TIME_SECS = 60\n",
    "    ts_index_resampled = np.linspace(0, MAXIMUM_TIME_SECS, 60 * FS) # The way used by the authors of the dataset. I would use `np.arange(0,60,1/FS)`\n",
    "    df_peaks = pd.DataFrame(data=np.zeros(ts_index_resampled.size, dtype=int), index=ts_index_resampled, columns=[\"IBI_IBI\"])\n",
    "\n",
    "    # Match the IBI times to the closest timestamp in the array containing the peaks\n",
    "    closest_times_to_peaks = df_peaks.index.get_indexer(df.index.values, method=\"nearest\")\n",
    "    closest_index_values = df_peaks.index[closest_times_to_peaks] # Get index values from the positions\n",
    "    df_peaks.loc[closest_index_values] = 1\n",
    "\n",
    "    # The dataframe needs to be called `R_Peaks` to extract HRV with neurokit\n",
    "    df_peaks = df_peaks.rename({ibi_colname:output_colname}, axis=1)\n",
    "    \n",
    "    return df_peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where the compiled dataset will be stored\n",
    "DATASET_POSTPROCESSED_WITH_RPEAKS_FILENAME = gen_path_temp(\"Dataset_CEAP_replacing_IBI_with_RPeaks\", extension=\".csv\")\n",
    "\n",
    "try:\n",
    "    df_ceap_with_Rpeaks = pd.read_csv(DATASET_POSTPROCESSED_WITH_RPEAKS_FILENAME)\n",
    "    print(\"Data loaded from file\")\n",
    "except:\n",
    "    print(\"Creating file\")\n",
    "    df_ceap_with_Rpeaks = df_ceap\n",
    "\n",
    "    # Add empty R_Peaks to the whole dataset\n",
    "    df_ceap_with_Rpeaks[r_peaks_colname] = np.nan\n",
    "\n",
    "    # Iterate over participants and videos to add the respective R_peaks\n",
    "    for pid in np.sort(df_ceap_with_Rpeaks.ParticipantID.unique()):\n",
    "        for vid in np.sort(df_ceap_with_Rpeaks.VideoID.unique()):\n",
    "            #######\n",
    "            # Query to filter subset of IBI data\n",
    "            Q = ( (data_IBI_raw.ParticipantID == pid) \n",
    "                & (data_IBI_raw.VideoID == vid)\n",
    "                & (data_IBI_raw.data_type == \"Physio\"))\n",
    "            # Extract the R_peaks from the corresponding Raw IBI\n",
    "            data_single_instance = data_IBI_raw[Q][[ts_colname, ibi_colname]].set_index(ts_colname).dropna(axis=0, how=\"all\")\n",
    "\n",
    "            # If contains the column, and the column has data\n",
    "            if(ibi_colname in data_single_instance.columns and data_single_instance[ibi_colname].size > 0):\n",
    "                data_peaks_resampled = extract_peaks_from_IBI(data_single_instance,\n",
    "                                                        FS=RESAMPLING_FREQUENCY,\n",
    "                                                        ibi_colname=ibi_colname,\n",
    "                                                        output_colname=r_peaks_colname)\n",
    "            else:\n",
    "                # Dataframe full of zeros but without peaks, to compensate for those samples without IBI data.\n",
    "                data_peaks_resampled = pd.DataFrame({\n",
    "                    ts_colname: timestamps_reference,\n",
    "                    r_peaks_colname: np.zeros(timestamps_reference.size, dtype=int),\n",
    "                }).set_index(ts_colname)\n",
    "\n",
    "            #######\n",
    "            # Replace the relevant subsection of the postprocessed data\n",
    "            # Query to filter subset of big dataframe\n",
    "            Q = ((df_ceap_with_Rpeaks.ParticipantID == pid)\n",
    "                & (df_ceap_with_Rpeaks.VideoID == vid)\n",
    "                & (df_ceap_with_Rpeaks.data_type == \"Physio\"))\n",
    "            idx_to_replace = df_ceap_with_Rpeaks[Q].index\n",
    "            df_ceap_with_Rpeaks.loc[idx_to_replace, r_peaks_colname ] = data_peaks_resampled[r_peaks_colname].values\n",
    "            print(f\"P{pid} - V{vid} - #R-Peaks:{data_peaks_resampled[r_peaks_colname].values.sum()}\")\n",
    "\n",
    "    ## Saving .csv\n",
    "    df_ceap_with_Rpeaks.to_csv( DATASET_POSTPROCESSED_WITH_RPEAKS_FILENAME, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ceap_with_Rpeaks.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example on how the peaks can be used in the feature extraction stage to calculate HRV with the package Neurokit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_IBI_one_participant = df_ceap_with_Rpeaks[ (df_ceap_with_Rpeaks[\"data_type\"] == \"Physio\") & \\\n",
    "                                                (df_ceap_with_Rpeaks[\"ParticipantID\"] == 1) & \\\n",
    "                                                (df_ceap_with_Rpeaks[\"VideoID\"] == 3) \\\n",
    "                                              ].dropna(axis=1, how=\"all\")\n",
    "data_IBI_one_participant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neurokit2 as nk\n",
    "x=data_IBI_one_participant[\"IBI_R_Peaks\"]\n",
    "hrv_indices = nk.hrv(x, sampling_rate=RESAMPLING_FREQUENCY, show=True)\n",
    "hrv_indices"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Organize the dataset ready for feature extraction\n",
    "---\n",
    "\n",
    "The last example shows how to organize the dataset for classification tasks:\n",
    "\n",
    "1. Remove NaN by merging the time-series per their respective `data_group`. They already have the same `ParticipantID`, `VideoID` and `TimeStamp`, thus it's easy to remove the column that indicates the type of data (*Annotations, Behavior, Physio*) so that the whole dataframe does not contain missing values.\n",
    "2. In this case, we define the class labels in high/low arousal/valence according to the video's reference V-A levels: `[HAHL, HALV, LAHV, LA,LV]`. However, the same procedure can be used to label the target classes in other ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dgroup_colname = \"data_type\"        # Existing column to be removed\n",
    "class_label_colname = \"class_VA\"    # Class column name to be created\n",
    "\n",
    "# These columns are used as index to join df\n",
    "basic_colnames = [participant_colname, video_colname, ts_colname]\n",
    "basic_colnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping used according to the paper's information in Table 1\n",
    "# doi: 10.1109/TMM.2021.3124080\n",
    "MAPPING_VIDEO_TO_CLASS = {\n",
    "    1: \"HVHA\",\n",
    "    2: \"HVLA\",\n",
    "    3: \"LVHA\",\n",
    "    4: \"LVLA\",\n",
    "    5: \"HVHA\",\n",
    "    6: \"HVLA\",\n",
    "    7: \"LVHA\",\n",
    "    8: \"LVLA\",\n",
    "}\n",
    "# Create a function from the dictionary to apply on the final array\n",
    "mapper_videoid_to_classes = np.vectorize(MAPPING_VIDEO_TO_CLASS.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where the compiled dataset will be stored\n",
    "DATASET_POSTPROCESSED_WITHOUT_NAN = gen_path_temp(\"Dataset_CEAP_postprocessed\", extension=\".csv\")\n",
    "\n",
    "# Load or create dataframe with statistics of initial dataset\n",
    "dataset_postprocessed_no_nan = None\n",
    "\n",
    "try:\n",
    "    dataset_postprocessed_no_nan = pd.read_csv(DATASET_POSTPROCESSED_WITHOUT_NAN)\n",
    "    print(\"Data loaded from file\")\n",
    "except:\n",
    "    print(\"Creating file\")\n",
    "\n",
    "    # Delete preprocessing level info (Full of labels saying `Frame`)\n",
    "    df_ceap_with_Rpeaks = df_ceap_with_Rpeaks.drop([\"processing_level\"], axis=1)\n",
    "\n",
    "    # Merge data from different groups to remove Nan values\n",
    "    for pid in np.sort(df_ceap_with_Rpeaks[participant_colname].unique()):\n",
    "        for vid in np.sort(df_ceap_with_Rpeaks[video_colname].unique()):\n",
    "            # Stores the different data groups per time-series instance.\n",
    "            df_instance = None\n",
    "            for dg in DATA_GROUPS:\n",
    "                print(f\"P{pid} V{vid} G:{dg}\")\n",
    "                Q = ( (df_ceap_with_Rpeaks[participant_colname] == pid) \n",
    "                    & (df_ceap_with_Rpeaks[video_colname] == vid)\n",
    "                    & (df_ceap_with_Rpeaks[dgroup_colname] == dg))\n",
    "                \n",
    "                selection_idx = df_ceap_with_Rpeaks[Q].index\n",
    "                data_per_group = df_ceap_with_Rpeaks.loc[selection_idx].copy()\n",
    "\n",
    "                # Load the data get the relevant columns that do not contain missing values\n",
    "                data_per_group.drop(dgroup_colname, axis=1, inplace=True)\n",
    "                data_per_group.set_index(basic_colnames, inplace=True)\n",
    "                data_per_group.dropna(axis=1, how=\"all\", inplace=True)\n",
    "\n",
    "                # Add specific data group to time series\n",
    "                df_instance = data_per_group if (df_instance is None) else df_instance.join(data_per_group)\n",
    "\n",
    "            # Add joined dataset to general one\n",
    "            df_instance.reset_index(inplace=True)\n",
    "            dataset_postprocessed_no_nan = df_instance if (dataset_postprocessed_no_nan is None) else pd.concat([dataset_postprocessed_no_nan, df_instance], axis=0, ignore_index=True)\n",
    "\n",
    "print(\"\\tEnd\")\n",
    "\n",
    "# Map each video to the corresponding Class label\n",
    "video_id_array = dataset_postprocessed_no_nan[video_colname]\n",
    "dataset_postprocessed_no_nan[class_label_colname] = mapper_videoid_to_classes(video_id_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target array (y)\n",
    "video_id_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data (X)\n",
    "dataset_postprocessed_no_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "dataset_postprocessed_no_nan.isna().sum(axis=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following steps may involve the transformation of each time series into tabular form with overlapping windows, or directly apply time-series classifiers..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples to work with the data as MultiIndex (easier to apply feature extraction with rolling windows)\n",
    "dataset_postprocessed_no_nan.set_index([\"ParticipantID\",\"VideoID\",\"TimeStamp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('End of notebook')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "5109d816b82be14675a6b11f8e0f0d2e80f029176ed3710d54e125caa8520dfd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
