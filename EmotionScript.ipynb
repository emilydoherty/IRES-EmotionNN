{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emotion_Datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "# import flirt\n",
    "# import flirt.reader.empatica\n",
    "# import flirt.with_\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VREED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Import VREED Dataset\n",
    "\n",
    "# #Questionnaires \n",
    "# VREEDpreQ = pd.read_excel('/Users/emilydoherty/Desktop/IRES/Emotion_Datasets/VREED/Self-Reported Questionnaires/Participant Profile  Pre-Exposure Ratings.xlsx')\n",
    "# VREEDpostQ=pd.read_excel('/Users/emilydoherty/Desktop/IRES/Emotion_Datasets/VREED/Self-Reported Questionnaires/Post Exposure Ratings.xlsx')\n",
    "\n",
    "# #Features (min, max, etc.)\n",
    "# VREEDeye=pd.read_csv('/Users/emilydoherty/Desktop/IRES/Emotion_Datasets/VREED/Eye Tracking Data/Eye Tracking Data (Features Extracted)/EyeTracking_FeaturesExtracted.csv')\n",
    "# VREEDecg=pd.read_csv('/Users/emilydoherty/Desktop/IRES/Emotion_Datasets/VREED/ECG-GSR Data/ECG-GSR (Features Extracted)/ECG_FeaturesExtracted.csv')\n",
    "# VREEDgsr=pd.read_csv('/Users/emilydoherty/Desktop/IRES/Emotion_Datasets/VREED/ECG-GSR Data/ECG-GSR (Features Extracted)/GSR_FeaturesExtracted.csv')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AVReality \n",
    "this dataset will need to be filtered - hopefully use same filtering techniques as CEAP to extract features once that script is debugged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Age:</th>\n",
       "      <th>Sex:</th>\n",
       "      <th>ERQ</th>\n",
       "      <th>N_Valence</th>\n",
       "      <th>N_Arousal</th>\n",
       "      <th>N_Dominance</th>\n",
       "      <th>N_EMFACS</th>\n",
       "      <th>N_Realism</th>\n",
       "      <th>N_Appeal</th>\n",
       "      <th>...</th>\n",
       "      <th>N_Trustworthy</th>\n",
       "      <th>S_Valence</th>\n",
       "      <th>S_Arousal</th>\n",
       "      <th>S_Dominance</th>\n",
       "      <th>S_EMFACS</th>\n",
       "      <th>S_Realism</th>\n",
       "      <th>S_Appeal</th>\n",
       "      <th>S_Familiar</th>\n",
       "      <th>S_Friendly</th>\n",
       "      <th>S_Trustworthy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S01</td>\n",
       "      <td>22</td>\n",
       "      <td>Male</td>\n",
       "      <td>Cognitive Reappraisal</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>Happiness</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>Disgust</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S02</td>\n",
       "      <td>22</td>\n",
       "      <td>Female</td>\n",
       "      <td>Cognitive Reappraisal</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>Happiness</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>Anger</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S03</td>\n",
       "      <td>19</td>\n",
       "      <td>Male</td>\n",
       "      <td>Expressive Suppression</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>Happiness</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>Sadness</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S04</td>\n",
       "      <td>27</td>\n",
       "      <td>Male</td>\n",
       "      <td>Expressive Suppression</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Happiness</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>Anger</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S05</td>\n",
       "      <td>20</td>\n",
       "      <td>Male</td>\n",
       "      <td>Cognitive Reappraisal</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>Sadness</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>Anger</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID  Age:    Sex:                     ERQ  N_Valence  N_Arousal  \\\n",
       "0  S01    22    Male  Cognitive Reappraisal           5          3   \n",
       "1  S02    22  Female  Cognitive Reappraisal           7          4   \n",
       "2  S03    19    Male  Expressive Suppression          7          4   \n",
       "3  S04    27    Male  Expressive Suppression          9          5   \n",
       "4  S05    20    Male  Cognitive Reappraisal           6          4   \n",
       "\n",
       "   N_Dominance   N_EMFACS  N_Realism  N_Appeal  ...  N_Trustworthy  S_Valence  \\\n",
       "0            7  Happiness          6         3  ...              7          4   \n",
       "1            6  Happiness          6         4  ...              4          4   \n",
       "2            6  Happiness          6         6  ...              5          4   \n",
       "3            3  Happiness          5         3  ...              4          3   \n",
       "4            9    Sadness          6         4  ...              3          7   \n",
       "\n",
       "   S_Arousal   S_Dominance  S_EMFACS  S_Realism S_Appeal  S_Familiar  \\\n",
       "0          3             7   Disgust          7        4           7   \n",
       "1          6             4     Anger          6        3           6   \n",
       "2          5             6   Sadness          6        6           6   \n",
       "3          6             7     Anger          4        6           7   \n",
       "4          6             9     Anger          6        4           5   \n",
       "\n",
       "   S_Friendly  S_Trustworthy  \n",
       "0           7              6  \n",
       "1           5              6  \n",
       "2           6              6  \n",
       "3           6              4  \n",
       "4           5              2  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AV Reality Data\n",
    "AVRq=pd.read_excel('/Users/emilydoherty/IRES-EmotionNN/AVReality/Data_new.xlsx',sheet_name=\"Sheet1\")\n",
    "AVRq['ID']=AVRq['ID'].astype(str)\n",
    "AVRq.describe()\n",
    "AVRq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nature GSR Files \n",
    "path = '/Users/emilydoherty/IRES-EmotionNN/AVReality/E4/Nature/GSR'\n",
    "csv_files = glob.glob(path + \"/*.csv\")\n",
    "csv_files=sorted(csv_files)\n",
    "\n",
    "import scipy.signal as signal\n",
    "\n",
    "# low-pass filter and normalization\n",
    "def SignalTrans(_signalList, order, cutoff):\n",
    "    wn = 2 * cutoff / 4\n",
    "    b, a = signal.butter(order, wn, 'lowpass')\n",
    "    filterY = signal.filtfilt(b, a, _signalList)\n",
    "\n",
    "    _range = np.max(filterY) - np.min(filterY)\n",
    "    filterY1 = (filterY - np.min(filterY)) / _range\n",
    "    return filterY1\n",
    "\n",
    "# EDA changes\n",
    "def EDA_Vary(_signalList, wn):  \n",
    "    _velocityList = [0]\n",
    "    for i in range(1, len(_signalList)):\n",
    "        _v = (_signalList[i] - _signalList[i - 1]) / wn\n",
    "        _velocityList.append(abs(_v))\n",
    "    return _velocityList\n",
    "\n",
    "# Z-score\n",
    "def z_score_normalization(x):\n",
    "    x = (x - np.mean(x)) / np.std(x)\n",
    "    return x\n",
    "\n",
    "# Read each CSV file into DataFrame\n",
    "# This creates a list of dataframes\n",
    "N_GSR=[]\n",
    "\n",
    "for file in csv_files:\n",
    "\tdf_list = (pd.read_csv(file))\n",
    "\tdf_list['filename']= os.path.basename(file)\n",
    "\tdf_list['cond']= 'Nature'\n",
    "\tdf_list['ID']= df_list['filename'].str.replace(\"_GSR_Nature.csv\",\" \")\n",
    "\tdf_list['filtered'] = SignalTrans(df_list['Channel 1'],3,0.5)\n",
    "\tdf_list['vary'] = EDA_Vary(df_list['filtered'],0.25)\n",
    "\tdf_list['mean_GSR']=np.mean(df_list['vary'])\n",
    "\t# df_list['zscore']=z_score_normalization(df_list['vary']) \n",
    "\tN_GSR.append(df_list)\n",
    "        \n",
    "\n",
    "N_GSR = pd.concat(N_GSR)\n",
    "N_GSR = N_GSR[['ID','cond','mean_GSR']].drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subway GSR Files \n",
    "path = '/Users/emilydoherty/IRES-EmotionNN/AVReality/E4/Subway/GSR'\n",
    "csv_files = glob.glob(path + \"/*.csv\")\n",
    "csv_files=sorted(csv_files)\n",
    "\n",
    "# Read each CSV file into DataFrame\n",
    "# This creates a list of dataframes\n",
    "S_GSR=[]\n",
    "\n",
    "for file in csv_files:\n",
    "\tdf_list = (pd.read_csv(file))\n",
    "\tdf_list['filename']= os.path.basename(file)\n",
    "\tdf_list['ID']= df_list['filename'].str.replace(\"_GSR.csv\",\" \")\n",
    "\tdf_list['cond']= 'Subway'\n",
    "\tdf_list['filtered'] = SignalTrans(df_list['Channel 1'],3,0.5)\n",
    "\tdf_list['vary'] = EDA_Vary(df_list['filtered'],0.25)\n",
    "\tdf_list['mean_GSR']=np.mean(df_list['vary'])\n",
    "\t# df_list['zscore']=z_score_normalization(df_list['vary']) \n",
    "\tS_GSR.append(df_list)\n",
    "        \n",
    "S_GSR = pd.concat(S_GSR)\n",
    "S_GSR = S_GSR[['ID','cond','mean_GSR']].drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All GSR data\n",
    "ALL_GSR= N_GSR.merge(S_GSR, on='ID')\n",
    "# N_GSR.set_index('ID')\n",
    "# AVRq.set_index('ID')\n",
    "# N_GSR2=N_GSR.merge(AVRq, left_on='ID', right_on='ID', how='right')\n",
    "# N_GSR2.to_excel('N_GSR2.xlsx')\n",
    "# N_GSR.to_excel('N_GSR.xlsx')\n",
    "# AVRq.to_excel('AVRq.xlsx')\n",
    "\n",
    "# ALL_GSR.to_excel('ALL_GSR.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nature BVP Files \n",
    "path = '/Users/emilydoherty/IRES-EmotionNN/AVReality/E4/Nature/BVP'\n",
    "csv_files = glob.glob(path + \"/*.csv\")\n",
    "csv_files=sorted(csv_files)\n",
    "\n",
    "# Read each CSV file into DataFrame\n",
    "# This creates a list of dataframes\n",
    "N_BVP=[]\n",
    "for file in csv_files:\n",
    "\tdf_list = (pd.read_csv(file))\n",
    "\tdf_list['filename']= os.path.basename(file)\n",
    "\tdf_list['ID']= df_list['filename'].str.replace(\"_BVP_Nature.csv\",\" \")\n",
    "\tdf_list['cond']= 'Nature'\n",
    "\tdf_list['filtered'] = SignalTrans(df_list['Channel 1'],3,0.5)\n",
    "\tdf_list['mean_BVP']=np.mean(df_list['filtered'])\n",
    "\t# df_list['zscore']=z_score_normalization(df_list['filtered']) \n",
    "\tN_BVP.append(df_list)\n",
    "\t\n",
    "N_BVP = pd.concat(N_BVP)\n",
    "N_BVP = N_BVP[['ID','cond','mean_BVP']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subway BVP Files \n",
    "path = '/Users/emilydoherty/IRES-EmotionNN/AVReality/E4/Subway/BVP'\n",
    "csv_files = glob.glob(path + \"/*.csv\")\n",
    "csv_files=sorted(csv_files)\n",
    "\n",
    "# Read each CSV file into DataFrame\n",
    "# This creates a list of dataframes\n",
    "S_BVP=[]\n",
    "for file in csv_files:\n",
    "\tdf_list = (pd.read_csv(file))\n",
    "\tdf_list['filename']= os.path.basename(file)\n",
    "\tdf_list['ID']= df_list['filename'].str.replace(\"_BVP.csv\",\" \")\n",
    "\tdf_list['cond']= 'Subway'\n",
    "\tdf_list['filtered'] = SignalTrans(df_list['Channel 1'],3,0.5)\n",
    "\tdf_list['mean_BVP']=np.mean(df_list['filtered'])\n",
    "\tdf_list['zscore']=z_score_normalization(df_list['filtered']) \n",
    "\tS_BVP.append(df_list)\n",
    "\t\n",
    "S_BVP = pd.concat(S_BVP)\n",
    "S_BVP = S_BVP[['ID','cond','mean_BVP']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All Blood Vol Pulse data\n",
    "ALL_BVP= N_BVP.merge(S_BVP, on='ID')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s5/2vb14g7n453_16xv4yf0wrv40000gn/T/ipykernel_86835/595981692.py:15: RuntimeWarning: invalid value encountered in divide\n",
      "  filterY1 = (filterY - np.min(filterY)) / _range\n"
     ]
    }
   ],
   "source": [
    "#Nature TMP Files \n",
    "path = '/Users/emilydoherty/IRES-EmotionNN/AVReality/E4/Nature/TMP'\n",
    "csv_files = glob.glob(path + \"/*.csv\")\n",
    "csv_files=sorted(csv_files)\n",
    "\n",
    "# Read each CSV file into DataFrame\n",
    "# This creates a list of dataframes\n",
    "N_TMP=[]\n",
    "for file in csv_files:\n",
    "\tdf_list = (pd.read_csv(file))\n",
    "\tdf_list['filename']= os.path.basename(file)\n",
    "\tdf_list['ID']= df_list['filename'].str.replace(\"_TMP_Nature.csv\",\" \")\n",
    "\tdf_list['cond']= 'Nature'\n",
    "\tdf_list['filtered'] = SignalTrans(df_list['Channel 1'],3,0.5)\n",
    "\tdf_list['mean_TMP']=np.mean(df_list['filtered'])\n",
    "\tdf_list['zscore']=z_score_normalization(df_list['filtered']) \n",
    "\tN_TMP.append(df_list)\n",
    "\t\n",
    "N_TMP = pd.concat(N_TMP)\n",
    "N_TMP = N_TMP[['ID','cond','mean_TMP']].drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s5/2vb14g7n453_16xv4yf0wrv40000gn/T/ipykernel_86835/595981692.py:15: RuntimeWarning: invalid value encountered in divide\n",
      "  filterY1 = (filterY - np.min(filterY)) / _range\n"
     ]
    }
   ],
   "source": [
    "#Subway TMP Files \n",
    "path = '/Users/emilydoherty/IRES-EmotionNN/AVReality/E4/Subway/TMP'\n",
    "csv_files = glob.glob(path + \"/*.csv\")\n",
    "csv_files=sorted(csv_files)\n",
    "\n",
    "# Read each CSV file into DataFrame\n",
    "# This creates a list of dataframes\n",
    "S_TMP=[]\n",
    "for file in csv_files:\n",
    "\tdf_list = (pd.read_csv(file))\n",
    "\tdf_list['filename']= os.path.basename(file)\n",
    "\tdf_list['ID']= df_list['filename'].str.replace(\"_TMP.csv\",\" \")\n",
    "\t# df_list['ID']= df_list['filename'].str.split(\"_TMP.csv\",n=1,expand = False)\n",
    "\tdf_list['cond']= 'Subway'\n",
    "\tdf_list['filtered'] = SignalTrans(df_list['Channel 1'],3,0.5)\n",
    "\tdf_list['mean_TMP']=np.mean(df_list['filtered'])\n",
    "\tdf_list['zscore']=z_score_normalization(df_list['filtered']) \n",
    "\tS_TMP.append(df_list)\n",
    "\t\n",
    "S_TMP = pd.concat(S_TMP)\n",
    "S_TMP = S_TMP[['ID','cond','mean_TMP']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All TMP data\n",
    "ALL_TMP= N_TMP.merge(S_TMP, on='ID')\n",
    "N_ALL=N_TMP.merge(N_GSR, on=['ID','cond']).merge(N_BVP,on=['ID','cond'])\n",
    "N_ALL.to_excel('N_ALL.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_ALL=S_TMP.merge(S_GSR, on=['ID','cond']).merge(S_BVP,on=['ID','cond'])\n",
    "S_ALL.to_excel('S_ALL.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_ALL=pd.read_excel('/Users/emilydoherty/IRES-EmotionNN/AVReality/S_ALL copy.xlsx')\n",
    "N_ALL=pd.read_excel('/Users/emilydoherty/IRES-EmotionNN/AVReality/N_ALL copy.xlsx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###CEAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "###RUN this in terminal   LEFT OFF HERE TRYING TO RUN THIS SCRIPT\n",
    "\n",
    "# cd '/Users/emilydoherty/Desktop/IRES/Emotion_Datasets/CEAP-360VR-Dataset-master/CEAP-360VR/6_Scripts/2_Data Processed'\n",
    "# python3 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import CEAP Data\n",
    "\n",
    "# Post-processed .csv\n",
    "# CEAPpost=pd.read_csv('/Users/emilydoherty/Desktop/Data Jupyter Notebook_NEW/temp/ceap_example/Dataset_CEAP_postprocessed.csv')\n",
    "\n",
    "#Load CEAP Features .csv\n",
    "#got these features by editing script to extract features in the CEAP folder, include mean/med/std of each measure \n",
    "# Get CSV files list from a folder\n",
    "path = '/Users/emilydoherty/IRES-EmotionNN/CEAP360/Features/'\n",
    "csv_files = glob.glob(path + \"*.csv\")\n",
    "\n",
    "# Read each CSV file into DataFrame\n",
    "# This creates a list of dataframes\n",
    "df_list = (pd.read_csv(file) for file in csv_files)\n",
    "\n",
    "# Concatenate all DataFrames into one \n",
    "CEAPfeatures= pd.concat(df_list, ignore_index=True)\n",
    "CEAPfeatures_parsed = CEAPfeatures[['EDA_Mean', 'SKT_Mean', 'BVP_Mean', 'HR_Mean' ,'V_binary']].copy() ###continue here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EDA_Mean</th>\n",
       "      <th>SKT_Mean</th>\n",
       "      <th>BVP_Mean</th>\n",
       "      <th>HR_Mean</th>\n",
       "      <th>V_binary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3072.000000</td>\n",
       "      <td>3072.000000</td>\n",
       "      <td>3072.000000</td>\n",
       "      <td>3072.000000</td>\n",
       "      <td>3072.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-4.702913</td>\n",
       "      <td>5.149819</td>\n",
       "      <td>-6.704183</td>\n",
       "      <td>5.273288</td>\n",
       "      <td>0.594727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>10.950086</td>\n",
       "      <td>2.259456</td>\n",
       "      <td>48.025534</td>\n",
       "      <td>2.485801</td>\n",
       "      <td>0.491025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-69.528895</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-152.907475</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-9.513263</td>\n",
       "      <td>3.781896</td>\n",
       "      <td>-31.596526</td>\n",
       "      <td>3.447941</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-4.472074</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>-2.395364</td>\n",
       "      <td>5.026945</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.431612</td>\n",
       "      <td>6.606881</td>\n",
       "      <td>17.500847</td>\n",
       "      <td>7.383758</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>65.856645</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>169.225876</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          EDA_Mean     SKT_Mean     BVP_Mean      HR_Mean     V_binary\n",
       "count  3072.000000  3072.000000  3072.000000  3072.000000  3072.000000\n",
       "mean     -4.702913     5.149819    -6.704183     5.273288     0.594727\n",
       "std      10.950086     2.259456    48.025534     2.485801     0.491025\n",
       "min     -69.528895     1.000000  -152.907475     1.000000     0.000000\n",
       "25%      -9.513263     3.781896   -31.596526     3.447941     0.000000\n",
       "50%      -4.472074     5.000000    -2.395364     5.026945     1.000000\n",
       "75%       0.431612     6.606881    17.500847     7.383758     1.000000\n",
       "max      65.856645     9.000000   169.225876     9.000000     1.000000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CEAPfeatures_parsed.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VRFS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#maybe\n",
    "#run in terminal : pip install flirt\n",
    "#EDA \n",
    "from glob import glob as glob\n",
    "paths = [\"/Users/emilydoherty/IRES-EmotionNN/VRFS/Data/Physiological Signal/Phasmophobia\", \"/Users/emilydoherty/IRES-EmotionNN/VRFS/Data/Physiological Signal/Warthunder\", \"/Users/emilydoherty/IRES-EmotionNN/VRFS/Data/Physiological Signal/Minecraft\", \"/Users/emilydoherty/IRES-EmotionNN/VRFS/Data/Physiological Signal/Dirt Rally\"]\n",
    "eda_VR = []\n",
    "for path in paths:\n",
    "  for participant in glob(path + \"/*/VR\", recursive = True):\n",
    "    p = glob(participant+\"/EDA.csv\")[0]\n",
    "    # eda_VR.append(flirt.reader.empatica.read_eda_file_into_df(p)[\"eda\"])\n",
    "    df_list = (pd.read_csv(p))\n",
    "    df_list['PID']=participant\n",
    "    df_list.rename(columns={df_list.columns[0]: 'GSR'},inplace=True)\n",
    "    df_list['filtered'] = SignalTrans(df_list['GSR'],3,0.5)\n",
    "    df_list['vary'] = EDA_Vary(df_list['filtered'],0.25)\n",
    "    df_list['mean_GSR']=np.mean(df_list['vary'])\n",
    "    eda_VR.append(df_list)\n",
    "        \n",
    "eda_VR = pd.concat(eda_VR)\n",
    "eda_VR = eda_VR[['mean_GSR']].drop_duplicates()\n",
    "eda_VR['ID']=[27, 28, 29, 30, 31, 32, 33, 34, 1, 2, 3, 4, 14, 24, 35, 8, 9, 13, 14, 15, 16, 17, 18, 19, 5, 6, 19, 20, 21, 22, 23, 25, 26]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BVP\n",
    "BVP_VR=[]\n",
    "for path in paths:\n",
    "\tfor participant in glob(path + \"/*/VR\", recursive = True):\n",
    "\t\tp = glob(participant+\"/BVP.csv\")[0]\n",
    "\t\tdf_list = (pd.read_csv(p))\n",
    "\t\tdf_list.rename(columns={df_list.columns[0]: 'BVP'},inplace=True)\t\n",
    "\t\tdf_list['filtered'] = SignalTrans(df_list['BVP'],3,0.5)\n",
    "\t\tdf_list['mean_BVP']=np.mean(df_list['filtered'])\n",
    "\t\tBVP_VR.append(df_list)\n",
    "\t\n",
    "BVP_VR = pd.concat(BVP_VR)\n",
    "BVP_VR = BVP_VR['mean_BVP'].drop_duplicates()\n",
    "\n",
    "BVP_VR['ID']=[27, 28, 29, 30, 31, 32, 33, 34, 1, 2, 3, 4, 14, 24, 35, 8, 9, 13, 14, 15, 16, 17, 18, 19, 5, 6, 19, 20, 21, 22, 23, 25, 26]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TMP\n",
    "TMP_VR=[]\n",
    "for path in paths:\n",
    "  for participant in glob(path + \"/*/VR\", recursive = True):\n",
    "    p = glob(participant+\"/TEMP.csv\")[0]\n",
    "    df_list = (pd.read_csv(p))\n",
    "    df_list.rename(columns={df_list.columns[0]: 'TMP'},inplace=True)\n",
    "    df_list['ID']= (participant.strip('/VR')).strip(path)\n",
    "    df_list['filtered'] = SignalTrans(df_list['TMP'],3,0.5)\n",
    "    df_list['mean_TMP']=np.mean(df_list['filtered'])\n",
    "    TMP_VR.append(df_list)\n",
    "\t\n",
    "TMP_VR = pd.concat(TMP_VR)\n",
    "TMP_VR = TMP_VR['mean_TMP'].drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "TMP_VR['ID']=[27, 28, 29, 30, 31, 32, 33, 34, 1, 2, 3, 4, 14, 24, 35, 8, 9, 13, 14, 15, 16, 17, 18, 19, 5, 6, 19, 20, 21, 22, 23, 25, 26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAM = pd.read_csv('/Users/emilydoherty/IRES-EmotionNN/VRFS/Data/Psychological Signal/SAM.csv')\n",
    "SAM = SAM[SAM.Type != 'FS']\n",
    "SAM['ID']=[5, 6, 19, 20, 21, 22, 23, 25, 26, 1, 2, 3, 4, 14, 24, 35,27, 28, 29, 30, 31, 32, 33, 34,8, 9, 13, 14, 15, 16, 17, 18, 19]\n",
    "SAM2 = pd.merge(SAM, eda_VR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ALL.drop(columns=N_ALL.columns[0], axis=1, inplace=True)\n",
    "N_ALL.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten, Dropout, RandomFlip\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ALL.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ALL=N_ALL.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer, make_column_selector\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = N_ALL.copy()\n",
    "#Need to binarize N_Valence  (median split)\n",
    "X[\"N_Valence_Binary\"] = (X.N_Valence<X.N_Valence.quantile()).replace({True:1, False:0})\n",
    "X = X.drop(['ID','cond','N_Valence'],axis=1)\n",
    "# Remove target\n",
    "y= X.pop('N_Valence_Binary') #lets try to classify valence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to see which variables are categorical vs. not for one-hot encoding\n",
    "X.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure all numerical values fall under np.number class\n",
    "# X.select_dtypes(include = np.number).head ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = make_column_transformer(\n",
    "    (StandardScaler(),\n",
    "     make_column_selector(dtype_include=np.number)),\n",
    "    (OneHotEncoder(sparse=False),\n",
    "     make_column_selector(dtype_include=object)),\n",
    ")\n",
    "\n",
    "X = preprocessor.fit_transform(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = [X_train.shape[1]]\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NN Code\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(64,activation='relu'))\n",
    "model1.add(Dense(64, activation='relu'))\n",
    "model1.add(Dense(1, activation='sigmoid'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, \n",
    "                        verbose=2, mode='auto', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(optimizer='adam',     loss='binary_crossentropy', \n",
    "    metrics=['accuracy'])\n",
    "\n",
    "history = model1.fit(X_train, y_train, \n",
    "    validation_data= (X_test,y_test),\n",
    "    epochs=100,\n",
    "    batch_size=128, \n",
    "    callbacks=[monitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import info5604_utils as util\n",
    "importlib.reload(util)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.plot_history(history, plot_type='both')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist=pd.DataFrame(history.history)\n",
    "hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = model.evaluate(X_train, y_train, verbose=0)\n",
    "test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model1.predict(test_dset)\n",
    "pred_label = np.argmax(pred, axis=1)\n",
    "cmat = confusion_matrix(y_test, pred_label, normalize='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.heatmap(cmat, yticklabels=category_labels, xticklabels=category_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
