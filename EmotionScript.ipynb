{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emotion_Datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VREED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import VREED Dataset\n",
    "\n",
    "#Questionnaires \n",
    "VREEDpreQ = pd.read_excel('/Users/emilydoherty/Desktop/IRES/Emotion_Datasets/VREED/Self-Reported Questionnaires/Participant Profile  Pre-Exposure Ratings.xlsx')\n",
    "VREEDpostQ=pd.read_excel('/Users/emilydoherty/Desktop/IRES/Emotion_Datasets/VREED/Self-Reported Questionnaires/Post Exposure Ratings.xlsx')\n",
    "\n",
    "#Features (min, max, etc.)\n",
    "VREEDeye=pd.read_csv('/Users/emilydoherty/Desktop/IRES/Emotion_Datasets/VREED/Eye Tracking Data/Eye Tracking Data (Features Extracted)/EyeTracking_FeaturesExtracted.csv')\n",
    "VREEDecg=pd.read_csv('/Users/emilydoherty/Desktop/IRES/Emotion_Datasets/VREED/ECG-GSR Data/ECG-GSR (Features Extracted)/ECG_FeaturesExtracted.csv')\n",
    "VREEDgsr=pd.read_csv('/Users/emilydoherty/Desktop/IRES/Emotion_Datasets/VREED/ECG-GSR Data/ECG-GSR (Features Extracted)/GSR_FeaturesExtracted.csv')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CEAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import CEAP Data\n",
    "#Refer to ceap_example_notebook\n",
    "\n",
    "# Post-processed .csv\n",
    "# CEAPpost=pd.read_csv('/Users/emilydoherty/Desktop/Data Jupyter Notebook_NEW/temp/ceap_example/Dataset_CEAP_postprocessed.csv')\n",
    "\n",
    "#Load CEAP Features .csv\n",
    "#got these features by editing script to extract features in the CEAP folder, include mean/med/std of each measure \n",
    "# Get CSV files list from a folder\n",
    "path = '/Users/emilydoherty/IRES-EmotionNN/CEAP360/Features/'\n",
    "csv_files = glob.glob(path + \"*.csv\")\n",
    "\n",
    "# Read each CSV file into DataFrame\n",
    "# This creates a list of dataframes\n",
    "df_list = (pd.read_csv(file) for file in csv_files)\n",
    "\n",
    "# Concatenate all DataFrames into one \n",
    "CEAPfeatures= pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "#can probably remove some variables like VID \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AVReality \n",
    "this dataset will need to be filtered - hopefully use same filtering techniques as CEAP to extract features once that script is debugged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Age:</th>\n",
       "      <th>Sex:</th>\n",
       "      <th>ERQ</th>\n",
       "      <th>N_Valence</th>\n",
       "      <th>N_Arousal</th>\n",
       "      <th>N_Dominance</th>\n",
       "      <th>N_EMFACS</th>\n",
       "      <th>N_Realism</th>\n",
       "      <th>N_Appeal</th>\n",
       "      <th>...</th>\n",
       "      <th>N_Trustworthy</th>\n",
       "      <th>S_Valence</th>\n",
       "      <th>S_Arousal</th>\n",
       "      <th>S_Dominance</th>\n",
       "      <th>S_EMFACS</th>\n",
       "      <th>S_Realism</th>\n",
       "      <th>S_Appeal</th>\n",
       "      <th>S_Familiar</th>\n",
       "      <th>S_Friendly</th>\n",
       "      <th>S_Trustworthy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S01</td>\n",
       "      <td>22</td>\n",
       "      <td>Male</td>\n",
       "      <td>Cognitive Reappraisal</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>Happiness</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>Disgust</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S02</td>\n",
       "      <td>22</td>\n",
       "      <td>Female</td>\n",
       "      <td>Cognitive Reappraisal</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>Happiness</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>Anger</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S03</td>\n",
       "      <td>19</td>\n",
       "      <td>Male</td>\n",
       "      <td>Expressive Suppression</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>Happiness</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>Sadness</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S04</td>\n",
       "      <td>27</td>\n",
       "      <td>Male</td>\n",
       "      <td>Expressive Suppression</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Happiness</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>Anger</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S05</td>\n",
       "      <td>20</td>\n",
       "      <td>Male</td>\n",
       "      <td>Cognitive Reappraisal</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>Sadness</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>Anger</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID  Age:    Sex:                     ERQ  N_Valence  N_Arousal  \\\n",
       "0  S01    22    Male  Cognitive Reappraisal           5          3   \n",
       "1  S02    22  Female  Cognitive Reappraisal           7          4   \n",
       "2  S03    19    Male  Expressive Suppression          7          4   \n",
       "3  S04    27    Male  Expressive Suppression          9          5   \n",
       "4  S05    20    Male  Cognitive Reappraisal           6          4   \n",
       "\n",
       "   N_Dominance   N_EMFACS  N_Realism  N_Appeal  ...  N_Trustworthy  S_Valence  \\\n",
       "0            7  Happiness          6         3  ...              7          4   \n",
       "1            6  Happiness          6         4  ...              4          4   \n",
       "2            6  Happiness          6         6  ...              5          4   \n",
       "3            3  Happiness          5         3  ...              4          3   \n",
       "4            9    Sadness          6         4  ...              3          7   \n",
       "\n",
       "   S_Arousal   S_Dominance  S_EMFACS  S_Realism S_Appeal  S_Familiar  \\\n",
       "0          3             7   Disgust          7        4           7   \n",
       "1          6             4     Anger          6        3           6   \n",
       "2          5             6   Sadness          6        6           6   \n",
       "3          6             7     Anger          4        6           7   \n",
       "4          6             9     Anger          6        4           5   \n",
       "\n",
       "   S_Friendly  S_Trustworthy  \n",
       "0           7              6  \n",
       "1           5              6  \n",
       "2           6              6  \n",
       "3           6              4  \n",
       "4           5              2  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AV Reality Data\n",
    "AVRq=pd.read_excel('/Users/emilydoherty/IRES-EmotionNN/AVReality/Data_new.xlsx',sheet_name=\"Sheet1\")\n",
    "AVRq['ID']=AVRq['ID'].astype(str)\n",
    "AVRq.describe()\n",
    "AVRq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nature GSR Files \n",
    "path = '/Users/emilydoherty/IRES-EmotionNN/AVReality/E4/Nature/GSR'\n",
    "csv_files = glob.glob(path + \"/*.csv\")\n",
    "csv_files=sorted(csv_files)\n",
    "\n",
    "import scipy.signal as signal\n",
    "\n",
    "# low-pass filter and normalization\n",
    "def SignalTrans(_signalList, order, cutoff):\n",
    "    wn = 2 * cutoff / 4\n",
    "    b, a = signal.butter(order, wn, 'lowpass')\n",
    "    filterY = signal.filtfilt(b, a, _signalList)\n",
    "\n",
    "    _range = np.max(filterY) - np.min(filterY)\n",
    "    filterY1 = (filterY - np.min(filterY)) / _range\n",
    "    return filterY1\n",
    "\n",
    "# EDA changes\n",
    "def EDA_Vary(_signalList, wn):  \n",
    "    _velocityList = [0]\n",
    "    for i in range(1, len(_signalList)):\n",
    "        _v = (_signalList[i] - _signalList[i - 1]) / wn\n",
    "        _velocityList.append(abs(_v))\n",
    "    return _velocityList\n",
    "\n",
    "# Z-score\n",
    "def z_score_normalization(x):\n",
    "    x = (x - np.mean(x)) / np.std(x)\n",
    "    return x\n",
    "\n",
    "# Read each CSV file into DataFrame\n",
    "# This creates a list of dataframes\n",
    "N_GSR=[]\n",
    "\n",
    "for file in csv_files:\n",
    "\tdf_list = (pd.read_csv(file))\n",
    "\tdf_list['filename']= os.path.basename(file)\n",
    "\tdf_list['cond']= 'Nature'\n",
    "\tdf_list['ID']= df_list['filename'].str.replace(\"_GSR_Nature.csv\",\" \")\n",
    "\tdf_list['filtered'] = SignalTrans(df_list['Channel 1'],3,0.5)\n",
    "\tdf_list['vary'] = EDA_Vary(df_list['filtered'],0.25)\n",
    "\tdf_list['mean_GSR']=np.mean(df_list['vary'])\n",
    "\t# df_list['zscore']=z_score_normalization(df_list['vary']) \n",
    "\tN_GSR.append(df_list)\n",
    "        \n",
    "\n",
    "N_GSR = pd.concat(N_GSR)\n",
    "N_GSR = N_GSR[['ID','cond','mean_GSR']].drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subway GSR Files \n",
    "path = '/Users/emilydoherty/IRES-EmotionNN/AVReality/E4/Subway/GSR'\n",
    "csv_files = glob.glob(path + \"/*.csv\")\n",
    "csv_files=sorted(csv_files)\n",
    "\n",
    "# Read each CSV file into DataFrame\n",
    "# This creates a list of dataframes\n",
    "S_GSR=[]\n",
    "\n",
    "for file in csv_files:\n",
    "\tdf_list = (pd.read_csv(file))\n",
    "\tdf_list['filename']= os.path.basename(file)\n",
    "\tdf_list['ID']= df_list['filename'].str.replace(\"_GSR.csv\",\" \")\n",
    "\tdf_list['cond']= 'Subway'\n",
    "\tdf_list['filtered'] = SignalTrans(df_list['Channel 1'],3,0.5)\n",
    "\tdf_list['vary'] = EDA_Vary(df_list['filtered'],0.25)\n",
    "\tdf_list['mean_GSR']=np.mean(df_list['vary'])\n",
    "\tdf_list['zscore']=z_score_normalization(df_list['vary']) \n",
    "\tS_GSR.append(df_list)\n",
    "        \n",
    "S_GSR = pd.concat(S_GSR)\n",
    "S_GSR = S_GSR[['ID','cond','mean_GSR']].drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All GSR data\n",
    "ALL_GSR= N_GSR.merge(S_GSR, on='ID')\n",
    "# N_GSR.set_index('ID')\n",
    "# AVRq.set_index('ID')\n",
    "# N_GSR2=N_GSR.merge(AVRq, left_on='ID', right_on='ID', how='right')\n",
    "# N_GSR2.to_excel('N_GSR2.xlsx')\n",
    "# N_GSR.to_excel('N_GSR.xlsx')\n",
    "# AVRq.to_excel('AVRq.xlsx')\n",
    "\n",
    "# ALL_GSR.to_excel('ALL_GSR.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nature BVP Files \n",
    "path = '/Users/emilydoherty/IRES-EmotionNN/AVReality/E4/Nature/BVP'\n",
    "csv_files = glob.glob(path + \"/*.csv\")\n",
    "csv_files=sorted(csv_files)\n",
    "\n",
    "# Read each CSV file into DataFrame\n",
    "# This creates a list of dataframes\n",
    "N_BVP=[]\n",
    "for file in csv_files:\n",
    "\tdf_list = (pd.read_csv(file))\n",
    "\tdf_list['filename']= os.path.basename(file)\n",
    "\tdf_list['ID']= df_list['filename'].str.replace(\"_BVP_Nature.csv\",\" \")\n",
    "\tdf_list['cond']= 'Nature'\n",
    "\tdf_list['filtered'] = SignalTrans(df_list['Channel 1'],3,0.5)\n",
    "\tdf_list['mean_BVP']=np.mean(df_list['filtered'])\n",
    "\tdf_list['zscore']=z_score_normalization(df_list['filtered']) \n",
    "\tN_BVP.append(df_list)\n",
    "\t\n",
    "N_BVP = pd.concat(N_BVP)\n",
    "N_BVP = N_BVP[['ID','cond','mean_BVP']].drop_duplicates()\n",
    "\n",
    "##How to get HR from BVP? -- calculate IBI and transform to beats/min \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subway BVP Files \n",
    "path = '/Users/emilydoherty/IRES-EmotionNN/AVReality/E4/Subway/BVP'\n",
    "csv_files = glob.glob(path + \"/*.csv\")\n",
    "csv_files=sorted(csv_files)\n",
    "\n",
    "# Read each CSV file into DataFrame\n",
    "# This creates a list of dataframes\n",
    "S_BVP=[]\n",
    "for file in csv_files:\n",
    "\tdf_list = (pd.read_csv(file))\n",
    "\tdf_list['filename']= os.path.basename(file)\n",
    "\tdf_list['ID']= df_list['filename'].str.replace(\"_BVP.csv\",\" \")\n",
    "\tdf_list['cond']= 'Subway'\n",
    "\tdf_list['filtered'] = SignalTrans(df_list['Channel 1'],3,0.5)\n",
    "\tdf_list['mean_BVP']=np.mean(df_list['filtered'])\n",
    "\tdf_list['zscore']=z_score_normalization(df_list['filtered']) \n",
    "\tS_BVP.append(df_list)\n",
    "\t\n",
    "S_BVP = pd.concat(S_BVP)\n",
    "S_BVP = S_BVP[['ID','cond','mean_BVP']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All Blood Vol Pulse data\n",
    "ALL_BVP= N_BVP.merge(S_BVP, on='ID')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s5/2vb14g7n453_16xv4yf0wrv40000gn/T/ipykernel_47886/595981692.py:15: RuntimeWarning: invalid value encountered in divide\n",
      "  filterY1 = (filterY - np.min(filterY)) / _range\n"
     ]
    }
   ],
   "source": [
    "#Nature TMP Files \n",
    "path = '/Users/emilydoherty/IRES-EmotionNN/AVReality/E4/Nature/TMP'\n",
    "csv_files = glob.glob(path + \"/*.csv\")\n",
    "csv_files=sorted(csv_files)\n",
    "\n",
    "# Read each CSV file into DataFrame\n",
    "# This creates a list of dataframes\n",
    "N_TMP=[]\n",
    "for file in csv_files:\n",
    "\tdf_list = (pd.read_csv(file))\n",
    "\tdf_list['filename']= os.path.basename(file)\n",
    "\tdf_list['ID']= df_list['filename'].str.replace(\"_TMP_Nature.csv\",\" \")\n",
    "\tdf_list['cond']= 'Nature'\n",
    "\tdf_list['filtered'] = SignalTrans(df_list['Channel 1'],3,0.5)\n",
    "\tdf_list['mean_TMP']=np.mean(df_list['filtered'])\n",
    "\tdf_list['zscore']=z_score_normalization(df_list['filtered']) \n",
    "\tN_TMP.append(df_list)\n",
    "\t\n",
    "N_TMP = pd.concat(N_TMP)\n",
    "N_TMP = N_TMP[['ID','cond','mean_TMP']].drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s5/2vb14g7n453_16xv4yf0wrv40000gn/T/ipykernel_47886/595981692.py:15: RuntimeWarning: invalid value encountered in divide\n",
      "  filterY1 = (filterY - np.min(filterY)) / _range\n"
     ]
    }
   ],
   "source": [
    "#Subway TMP Files \n",
    "path = '/Users/emilydoherty/IRES-EmotionNN/AVReality/E4/Subway/TMP'\n",
    "csv_files = glob.glob(path + \"/*.csv\")\n",
    "csv_files=sorted(csv_files)\n",
    "\n",
    "# Read each CSV file into DataFrame\n",
    "# This creates a list of dataframes\n",
    "S_TMP=[]\n",
    "for file in csv_files:\n",
    "\tdf_list = (pd.read_csv(file))\n",
    "\tdf_list['filename']= os.path.basename(file)\n",
    "\tdf_list['ID']= df_list['filename'].str.replace(\"_TMP.csv\",\" \")\n",
    "\t# df_list['ID']= df_list['filename'].str.split(\"_TMP.csv\",n=1,expand = False)\n",
    "\tdf_list['cond']= 'Subway'\n",
    "\tdf_list['filtered'] = SignalTrans(df_list['Channel 1'],3,0.5)\n",
    "\tdf_list['mean_TMP']=np.mean(df_list['filtered'])\n",
    "\tdf_list['zscore']=z_score_normalization(df_list['filtered']) \n",
    "\tS_TMP.append(df_list)\n",
    "\t\n",
    "S_TMP = pd.concat(S_TMP)\n",
    "S_TMP = S_TMP[['ID','cond','mean_TMP']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All TMP data\n",
    "ALL_TMP= N_TMP.merge(S_TMP, on='ID')\n",
    "N_ALL=N_TMP.merge(N_GSR, on=['ID','cond']).merge(N_BVP,on=['ID','cond'])\n",
    "N_ALL.to_excel('N_ALL.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_ALL=S_TMP.merge(S_GSR, on=['ID','cond']).merge(S_BVP,on=['ID','cond'])\n",
    "S_ALL.to_excel('S_ALL.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_ALL=pd.read_excel('/Users/emilydoherty/IRES-EmotionNN/AVReality/S_ALL copy.xlsx')\n",
    "N_ALL=pd.read_excel('/Users/emilydoherty/IRES-EmotionNN/AVReality/N_ALL copy.xlsx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VRFS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#maybe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>cond</th>\n",
       "      <th>mean_TMP</th>\n",
       "      <th>mean_GSR</th>\n",
       "      <th>mean_BVP</th>\n",
       "      <th>Age:</th>\n",
       "      <th>Sex:</th>\n",
       "      <th>ERQ</th>\n",
       "      <th>N_Valence</th>\n",
       "      <th>N_Arousal</th>\n",
       "      <th>N_Dominance</th>\n",
       "      <th>N_EMFACS</th>\n",
       "      <th>N_Realism</th>\n",
       "      <th>N_Appeal</th>\n",
       "      <th>N_Familiar</th>\n",
       "      <th>N_Friendly</th>\n",
       "      <th>N_Trustworthy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S01</td>\n",
       "      <td>Nature</td>\n",
       "      <td>0.633773</td>\n",
       "      <td>0.009576</td>\n",
       "      <td>0.601381</td>\n",
       "      <td>22</td>\n",
       "      <td>Male</td>\n",
       "      <td>Cognitive Reappraisal</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>Happiness</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S02</td>\n",
       "      <td>Nature</td>\n",
       "      <td>0.468205</td>\n",
       "      <td>0.020918</td>\n",
       "      <td>0.493360</td>\n",
       "      <td>22</td>\n",
       "      <td>Female</td>\n",
       "      <td>Cognitive Reappraisal</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>Happiness</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S03</td>\n",
       "      <td>Nature</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.875000</td>\n",
       "      <td>0.459229</td>\n",
       "      <td>19</td>\n",
       "      <td>Male</td>\n",
       "      <td>Expressive Suppression</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>Happiness</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S04</td>\n",
       "      <td>Nature</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.066246</td>\n",
       "      <td>0.458371</td>\n",
       "      <td>27</td>\n",
       "      <td>Male</td>\n",
       "      <td>Expressive Suppression</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Happiness</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S05</td>\n",
       "      <td>Nature</td>\n",
       "      <td>0.427848</td>\n",
       "      <td>0.008297</td>\n",
       "      <td>0.477126</td>\n",
       "      <td>20</td>\n",
       "      <td>Male</td>\n",
       "      <td>Cognitive Reappraisal</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>Sadness</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID    cond  mean_TMP  mean_GSR  mean_BVP  Age:    Sex:  \\\n",
       "0  S01   Nature  0.633773  0.009576  0.601381    22    Male   \n",
       "1  S02   Nature  0.468205  0.020918  0.493360    22  Female   \n",
       "2  S03   Nature  0.500000  1.875000  0.459229    19    Male   \n",
       "3  S04   Nature  0.500000  0.066246  0.458371    27    Male   \n",
       "4  S05   Nature  0.427848  0.008297  0.477126    20    Male   \n",
       "\n",
       "                      ERQ  N_Valence  N_Arousal  N_Dominance   N_EMFACS  \\\n",
       "0  Cognitive Reappraisal           5          3            7  Happiness   \n",
       "1  Cognitive Reappraisal           7          4            6  Happiness   \n",
       "2  Expressive Suppression          7          4            6  Happiness   \n",
       "3  Expressive Suppression          9          5            3  Happiness   \n",
       "4  Cognitive Reappraisal           6          4            9    Sadness   \n",
       "\n",
       "   N_Realism  N_Appeal  N_Familiar  N_Friendly  N_Trustworthy  \n",
       "0          6         3           6           7              7  \n",
       "1          6         4           5           5              4  \n",
       "2          6         6           5           6              5  \n",
       "3          5         3           1           7              4  \n",
       "4          6         4           4           5              3  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_ALL.drop(columns=N_ALL.columns[0], axis=1, inplace=True)\n",
    "N_ALL.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten, Dropout, RandomFlip\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 17)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_ALL.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer, make_column_selector\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = N_ALL.copy()\n",
    "# Remove target\n",
    "y = X.pop('N_EMFACS')\n",
    "\n",
    "preprocessor = make_column_transformer(\n",
    "    (StandardScaler(),\n",
    "     make_column_selector(dtype_include=np.number)),\n",
    "    (OneHotEncoder(sparse=False),\n",
    "     make_column_selector(dtype_include=object)),\n",
    ")\n",
    "\n",
    "X = preprocessor.fit_transform(X)\n",
    "# y = np.log(y) # log transform target instead of standardizing\n",
    "\n",
    "input_shape = [X.shape[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_shape' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 8\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mimport\u001b[39;00m keras\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m layers\n\u001b[1;32m      7\u001b[0m model \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mSequential([\n\u001b[0;32m----> 8\u001b[0m     layers\u001b[39m.\u001b[39mDense(\u001b[39m128\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m, input_shape\u001b[39m=\u001b[39minput_shape),\n\u001b[1;32m      9\u001b[0m     layers\u001b[39m.\u001b[39mDense(\u001b[39m128\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m),    \n\u001b[1;32m     10\u001b[0m     layers\u001b[39m.\u001b[39mDense(\u001b[39m64\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m     11\u001b[0m     layers\u001b[39m.\u001b[39mDense(\u001b[39m1\u001b[39m),\n\u001b[1;32m     12\u001b[0m ])\n\u001b[1;32m     14\u001b[0m model1 \u001b[39m=\u001b[39m Sequential()\n\u001b[1;32m     15\u001b[0m model1\u001b[39m.\u001b[39madd(Conv2D(\u001b[39m32\u001b[39m, (\u001b[39m3\u001b[39m, \u001b[39m3\u001b[39m), activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m, input_shape\u001b[39m=\u001b[39m(\u001b[39m32\u001b[39m, \u001b[39m32\u001b[39m, \u001b[39m3\u001b[39m)))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_shape' is not defined"
     ]
    }
   ],
   "source": [
    "#NN Code\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=input_shape),\n",
    "    layers.Dense(128, activation='relu'),    \n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1),\n",
    "])\n",
    "\n",
    "model1 = Sequential()\n",
    "model1.add(Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "model1.add(MaxPooling2D((2,2)))\n",
    "model1.add(Flatten())\n",
    "model1.add(Dense(64, activation='relu'))\n",
    "model1.add(Dense(10, activation='sigmoid'))\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, \n",
    "                        verbose=2, mode='auto', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(optimizer='adam', \n",
    "               loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history1 = model1.fit(train_dset, epochs=100, callbacks=[monitor], \n",
    "                      validation_data=test_dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.plot_history(history1, plot_type='both')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model1.predict(test_dset)\n",
    "pred_label = np.argmax(pred, axis=1)\n",
    "cmat = confusion_matrix(y_test, pred_label, normalize='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.heatmap(cmat, yticklabels=category_labels, xticklabels=category_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
